{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77eb96d",
   "metadata": {},
   "source": [
    "<img src = \"https://escp.eu/sites/default/files/logo/ESCP-logo-white-misalign.svg\" width = 400 style=\"background-color: #240085;\">\n",
    "<h1 align=center><font size = 6>ESCP Business School</font></h1>\n",
    "<h3 align=center><font size = 5>SCOR Datathon</font><br/>\n",
    "<font size = 3>The Data Science Challenge Bridging Indian Agricultureal Protection Gap</font></h3>\n",
    "<h6 align=center>Chapter 0 - Data Preparation</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6b29a",
   "metadata": {},
   "source": [
    "Last Updated: December 05, 2021\\\n",
    "Author: Group 21 - Anniek Brink, Jeanne Dubois, and Resha Dirga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71948c6c",
   "metadata": {},
   "source": [
    "<h3>Chapter Objectives</h3>\n",
    "\n",
    "<p>This chapter aims to prepare the initial dataset has been provided on this challenge. There are 80 files on the data repository, but mainly, they can be categorised into <b><u>six</u></b> agriculture datasets:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Kharif 2017 datasets</li>\n",
    "    <li>Rabi 2017 datasets</li>    \n",
    "    <li>Kharif 2018 datasets</li>\n",
    "    <li>Rabi 2018 datasets</li>\n",
    "    <li>Kharif 2019 datasets</li>\n",
    "    <li>Rabi 2019 datasets</li>\n",
    "</ul>\n",
    "\n",
    "The analysis moving forward will use these six datasets instead of individual files. Thus, the data will be merged before the data exploratory step. Also since there are three versions of dataset given (by year), this document will update the values in the dataset, using the earliest version as the base, then updating the values on the later version (if any).</p>\n",
    "\n",
    "<i>Please see <u>Datathon_2021_Glossary.docx</u> for the explanation of each column contained in the datasets.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddbb883",
   "metadata": {},
   "source": [
    "<h3>Chapter 1: Import modules</h3>\n",
    "<p>This chapter lists all modules that being used on this document. The module import process will be performed on this chapter</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b7e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c62b14",
   "metadata": {},
   "source": [
    "<h3>Chapter 2: Merge datasets by year and season</h3>\n",
    "<p>This chapter reads the 80 files and merge them into datasets based on the six categories. No pre-processing steps is done during this process (the values will be generated as-is).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb312df3",
   "metadata": {},
   "source": [
    "<h5>Chapter 2.1 - Kharif 2017 datasets and Rabi 2017 datasets</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c2c6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files and create dataframes from repository - Year 2017\n",
    "\n",
    "df_raw_kharif = {}\n",
    "df_raw_rabi = {}\n",
    "path = ('../../01_Repositories/Common_Repository/02_Data/2017/')\n",
    "filename_kharif = [\n",
    "    '2017_Andhra Pradesh_Kharif.xlsx',\n",
    "    '2017_Bihar_Kharif.xlsx',\n",
    "    '2017_Gujarat_Kharif.xlsx',\n",
    "    '2017_Haryana_Kharif.xlsx',\n",
    "    '2017_Jharkhand_Kharif.xlsx',\n",
    "    '2017_Karnataka_Kharif.xlsx',\n",
    "    '2017_Madhya Pradesh_Kharif.xlsx',\n",
    "    '2017_Maharashtra_Kharif.xlsx',\n",
    "    '2017_Odisha_Kharif.xlsx',\n",
    "    '2017_Rajasthan_Kharif.xlsx',\n",
    "    '2017_Telangana_Kharif.xlsx',\n",
    "    '2017_Uttar Pradesh_Kharif.xlsx',\n",
    "    '2017_West Bengal_Kharif.xlsx'\n",
    "]\n",
    "\n",
    "filename_rabi = [\n",
    "    '2017_Andhra Pradesh_Rabi.xlsx',\n",
    "    '2017_Bihar_Rabi.xlsx',\n",
    "    '2017_Chhattisgarh_Rabi.xlsx',\n",
    "    '2017_Gujarat_Rabi.xlsx',\n",
    "    '2017_Haryana_Rabi.xlsx',\n",
    "    '2017_Jharkhand_Rabi.xlsx',\n",
    "    '2017_Karnataka_Rabi.xlsx',\n",
    "    '2017_Madhya Pradesh_Rabi.xlsx',\n",
    "    '2017_Maharashtra_Rabi.xlsx',\n",
    "    '2017_Rajasthan_Rabi.xlsx',\n",
    "    '2017_Telangana_Rabi.xlsx',\n",
    "    '2017_Uttar Pradesh_Rabi.xlsx',\n",
    "    '2017_West Bengal_Rabi.xlsx'\n",
    "]\n",
    "\n",
    "for filename in filename_kharif:\n",
    "    path_file = pd.Series({1: path,2: filename}).str.cat()\n",
    "    df_raw_kharif[filename] = pd.read_excel (path_file)\n",
    "\n",
    "for filename in filename_rabi:\n",
    "    path_file = pd.Series({1: path, 2: filename}).str.cat()\n",
    "    df_raw_rabi[filename] = pd.read_excel (path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39601a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filenames and format\n",
    "kharif_merged_filename = \"datasets_merged/df_kharif_merged_2017.csv\"\n",
    "rabi_merged_filename = \"datasets_merged/df_rabi_merged_2017.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9edcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge States datasets and export to csv before data exploratory\n",
    "df_raw_merged = pd.concat(df_raw_kharif.values(), ignore_index=True)\n",
    "df_raw_merged.to_csv(kharif_merged_filename, sep=';')\n",
    "\n",
    "df_raw_merged = pd.concat(df_raw_rabi.values(), ignore_index=True)\n",
    "df_raw_merged.to_csv(rabi_merged_filename, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c13be4",
   "metadata": {},
   "source": [
    "<h5>Chapter 2.2 - Kharif 2018 datasets and Rabi 2018 datasets</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes from repository - Year 2018\n",
    "\n",
    "df_raw_kharif = {}\n",
    "df_raw_rabi = {}\n",
    "path = ('../../01_Repositories/Common_Repository/02_Data/2018/')\n",
    "filename_kharif = [\n",
    "    '2018_Andhra Pradesh_Kharif.xlsx',\n",
    "    '2018_Bihar_Kharif.xlsx',\n",
    "    '2018_Chhattisgarh_Kharif.xlsx',\n",
    "    '2018_Gujarat_Kharif.xlsx',\n",
    "    '2018_Haryana_Kharif.xlsx',\n",
    "    '2018_Jharkhand_Kharif.xlsx',\n",
    "    '2018_Karnataka_Kharif.xlsx',\n",
    "    '2018_Madhya Pradesh_Kharif.xlsx',\n",
    "    '2018_Maharashtra_Kharif.xlsx',\n",
    "    '2018_Odisha_Kharif.xlsx',\n",
    "    '2018_Rajasthan_Kharif.xlsx',\n",
    "    '2018_Telangana_Kharif.xlsx',\n",
    "    '2018_Uttar Pradesh_Kharif.xlsx',\n",
    "    '2018_West Bengal_Kharif.xlsx'\n",
    "]\n",
    "\n",
    "filename_rabi = [\n",
    "    '2018_Andhra Pradesh_Rabi.xlsx',\n",
    "    '2018_Bihar_Rabi.xlsx',\n",
    "    '2018_Chhattisgarh_Rabi.xlsx',\n",
    "    '2018_Gujarat_Rabi.xlsx',\n",
    "    '2018_Haryana_Rabi.xlsx',\n",
    "    '2018_Jharkhand_Rabi.xlsx',\n",
    "    '2018_Karnataka_Rabi.xlsx',\n",
    "    '2018_Madhya Pradesh_Rabi.xlsx',\n",
    "    '2018_Maharashtra_Rabi.xlsx',\n",
    "    '2018_Rajasthan_Rabi.xlsx',\n",
    "    '2018_Telangana_Rabi.xlsx',\n",
    "    '2018_Uttar Pradesh_Rabi.xlsx',\n",
    "    '2018_West Bengal_Rabi.xlsx'\n",
    "]\n",
    "\n",
    "for filename in filename_kharif:\n",
    "    path_file = pd.Series({1: path,2: filename}).str.cat()\n",
    "    df_raw_kharif[filename] = pd.read_excel (path_file)\n",
    "\n",
    "for filename in filename_rabi:\n",
    "    path_file = pd.Series({1: path, 2: filename}).str.cat()\n",
    "    df_raw_rabi[filename] = pd.read_excel (path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31249887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filenames and format\n",
    "kharif_merged_filename = \"datasets_merged/df_kharif_merged_2018.csv\"\n",
    "rabi_merged_filename = \"datasets_merged/df_rabi_merged_2018.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge States datasets and export to csv before data exploratory\n",
    "df_raw_merged = pd.concat(df_raw_kharif.values(), ignore_index=True)\n",
    "df_raw_merged.to_csv(kharif_merged_filename, sep=';')\n",
    "\n",
    "df_raw_merged = pd.concat(df_raw_rabi.values(), ignore_index=True)\n",
    "df_raw_merged.to_csv(rabi_merged_filename, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b75de",
   "metadata": {},
   "source": [
    "<h5>Chapter 2.3 - Kharif 2019 datasets and Rabi 2019 datasets</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31706ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes from repository - Year 2019\n",
    "\n",
    "df_raw_kharif = {}\n",
    "df_raw_rabi = {}\n",
    "path = ('../../01_Repositories/Common_Repository/02_Data/2019/')\n",
    "filename_kharif = [\n",
    "    '2019_Andhra Pradesh_Kharif.xlsx',\n",
    "    '2019_Chhattisgarh_Kharif.xlsx',\n",
    "    '2019_Gujarat_Kharif.xlsx',\n",
    "    '2019_Haryana_Kharif.xlsx',\n",
    "    '2019_Jharkhand_Kharif.xlsx',\n",
    "    '2019_Karnataka_Kharif.xlsx',\n",
    "    '2019_Madhya Pradesh_Kharif.xlsx',\n",
    "    '2019_Maharashtra_Kharif.xlsx',\n",
    "    '2019_Odisha_Kharif.xlsx',\n",
    "    '2019_Rajasthan_Kharif.xlsx',\n",
    "    '2019_Telangana_Kharif.xlsx',\n",
    "    '2019_Uttar Pradesh_Kharif.xlsx',\n",
    "    '2019_Uttarakhand_Kharif.xlsx',\n",
    "    '2019_West Bengal_Kharif.xlsx'\n",
    "]\n",
    "\n",
    "filename_rabi = [\n",
    "    '2019_Andhra Pradesh_Rabi.xlsx',\n",
    "    '2019_Chhattisgarh_Rabi.xlsx',\n",
    "    '2019_Gujarat_Rabi.xlsx',\n",
    "    '2019_Haryana_Rabi.xlsx',\n",
    "    '2019_Karnataka_Rabi.xlsx',\n",
    "    '2019_Madhya Pradesh_Rabi.xlsx',\n",
    "    '2019_Maharashtra_Rabi.xlsx',\n",
    "    '2019_Odisha_Rabi.xlsx',\n",
    "    '2019_Rajasthan_Rabi.xlsx',\n",
    "    '2019_Tamil Nadu_Rabi.xlsx',\n",
    "    '2019_Telangana_Rabi.xlsx',\n",
    "    '2019_Uttar Pradesh_Rabi.xlsx',\n",
    "    '2019_West Bengal_Rabi.xlsx'\n",
    "]\n",
    "\n",
    "for filename in filename_kharif:\n",
    "    path_file = pd.Series({1: path,2: filename}).str.cat()\n",
    "    df_raw_kharif[filename] = pd.read_excel (path_file)\n",
    "\n",
    "for filename in filename_rabi:\n",
    "    path_file = pd.Series({1: path, 2: filename}).str.cat()\n",
    "    df_raw_rabi[filename] = pd.read_excel (path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filenames and format\n",
    "kharif_merged_filename = \"datasets_merged/df_kharif_merged_2019.csv\"\n",
    "rabi_merged_filename = \"datasets_merged/df_rabi_merged_2019.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6209d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge States datasets and export to csv before data exploratory\n",
    "df_raw_merged = pd.concat(df_raw_kharif.values(), ignore_index=True)\n",
    "df_raw_merged.to_csv(kharif_merged_filename, sep=';')\n",
    "\n",
    "df_raw_merged = pd.concat(df_raw_rabi.values(), ignore_index=True)\n",
    "df_raw_merged.to_csv(rabi_merged_filename, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b86e3e",
   "metadata": {},
   "source": [
    "<h3>Chapter 3: Merge datasets to get the latest version of all data</h3>\n",
    "<p>This chapter reads the generated 6 datasets into two big datasets based on their season. Since there are three version of datasets (2017, 2018 and 2019), this chapter merge all three datasets into one for each season, assuming that the number of the year indicate the version of the datasets (2017 being the oldest and 2019 being the latest).</p>\n",
    "<p><b>Approach:</b><br/>\n",
    "The versioning will appreciate the later version more than its predecessor. Thus, 2017 dataset will be treated as a base dataset, which will be updated by 2018 dataset and 2019 dataset consecutively if there is an existing data pointon the older dataset (inplace). If there data point is new, then the new data point will be appended.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63ef23d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file directories\n",
    "merged_filenames = [\n",
    "    \"datasets_merged/df_kharif_merged_2017.csv\",\n",
    "    \"datasets_merged/df_rabi_merged_2017.csv\",\n",
    "    \"datasets_merged/df_kharif_merged_2018.csv\",\n",
    "    \"datasets_merged/df_rabi_merged_2018.csv\",\n",
    "    \"datasets_merged/df_kharif_merged_2019.csv\",\n",
    "    \"datasets_merged/df_rabi_merged_2019.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3badd702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store datasets in a dataframe\n",
    "df = {}\n",
    "for filename in merged_filenames:\n",
    "    df[filename] = pd.read_csv(filename, delimiter=\";\",index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a9ba4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for indexing the specific dataset\n",
    "def df_shorten(years, season):\n",
    "    if years == 2017 and season == \"kharif\":\n",
    "        return df[list(df.keys())[0]]\n",
    "    elif years == 2017 and season == \"rabi\":\n",
    "        return df[list(df.keys())[1]]\n",
    "    elif years == 2018 and season == \"kharif\":\n",
    "        return df[list(df.keys())[2]]\n",
    "    elif years == 2018 and season == \"rabi\":\n",
    "        return df[list(df.keys())[3]]\n",
    "    elif years == 2019 and season == \"kharif\":\n",
    "        return df[list(df.keys())[4]]\n",
    "    elif years == 2019 and season == \"rabi\":\n",
    "        return df[list(df.keys())[5]]\n",
    "    else:\n",
    "        print(\"Data is out of range. Available year data: 2017, 2018, 2019. Available season data: kharif, rabi (case sensitive)\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63579694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print datasets info\n",
    "for index in merged_filenames:\n",
    "    print('')\n",
    "    print(index)\n",
    "    print('----------')\n",
    "    df[index].info()\n",
    "    print('<<=====================>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a7f338",
   "metadata": {},
   "source": [
    "<p>There are some float columns with object datatype, i.e.:\n",
    "    <ul>\n",
    "        <li>Kharif 2017 dataset on '2010 Yield' column</li>\n",
    "        <li>Kharif 2018 dataset on '2010 Yield' column</li>\n",
    "        <li>Rabi 2017 dataset on 'Area Sown (Ha)' column</li>\n",
    "        <li>Rabi 2018 dataset on 'Area Sown (Ha)' column</li>\n",
    "    </ul>\n",
    "    \n",
    "Convert the datatype to numeric before proceeding with the next steps to prevent errors</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1cdc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datatype from object to float\n",
    "df_shorten(2017, \"kharif\")['2010 Yield'] = pd.to_numeric(df_shorten(2017, \"kharif\")['2010 Yield'], errors='coerce')\n",
    "df_shorten(2018, \"kharif\")['2010 Yield'] = pd.to_numeric(df_shorten(2018, \"kharif\")['2010 Yield'], errors='coerce')\n",
    "df_shorten(2017, \"rabi\")['Area Sown (Ha)'] = pd.to_numeric(df_shorten(2017, \"rabi\")['Area Sown (Ha)'], errors='coerce')\n",
    "df_shorten(2018, \"rabi\")['Area Sown (Ha)'] = pd.to_numeric(df_shorten(2018, \"rabi\")['Area Sown (Ha)'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50deb90e",
   "metadata": {},
   "source": [
    "<p>To make the identifier, values of which the columns used as the identifier need to be uniform. The unique features of each data points was defined by its administrative features (State, Cluster, Distruct, Sub-district, Block and GP), the Season and Crops. Thus, the following steps is to check and uniform the formatting and repair typos (if any).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b503444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a list containing name of columns to be included on ID\n",
    "cols_ID = ['State', 'Cluster', 'District', 'Sub-District', 'Block', 'GP', 'Season', 'Crop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707a985c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all identifier columns to lowercase string\n",
    "for season in [\"kharif\", \"rabi\"]:\n",
    "    for year in [2017, 2018, 2019]:\n",
    "        for col in cols_ID:\n",
    "            df_shorten(year, season)[col] = df_shorten(year, season)[col].apply(str)\n",
    "            df_shorten(year, season)[col] = df_shorten(year, season)[col].str.lower()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e867e9",
   "metadata": {},
   "source": [
    "<p>For the administration features, since there is a file for the prediction submission sheet, the values of the IDs will be matched according to the submission sheet</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "062fe434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store submission datasets in a dataframe\n",
    "submission_filenames = [\n",
    "    \"./../../03_Submission/03_Prediction/GP_Pred_Kharif.csv\",\n",
    "    \"./../../03_Submission/03_Prediction/GP_Pred_Rabi.csv\"\n",
    "]\n",
    "\n",
    "df_submit = {}\n",
    "for i in range(0, len(submission_filenames)):\n",
    "    df_submit[i] = pd.read_csv(submission_filenames[i],index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50bf3ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to check the similarities of values between columns\n",
    "def check_cols_similarities(data, col_list):\n",
    "    result = 1\n",
    "    for col in col_list[1:]:\n",
    "        if data[col] == data[0]:\n",
    "            result = result * 1\n",
    "        else:\n",
    "            result = result * 0\n",
    "    \n",
    "    if result == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e2c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table between data version 2017, 2018, 2019 and submission file\n",
    "def compare_table(column_name, season):\n",
    "    if season == \"kharif\":\n",
    "        i = 0\n",
    "    else:\n",
    "        i = 1\n",
    "\n",
    "    reference = {\n",
    "        'Reference': df_submit[i][column_name].unique(),\n",
    "        'Col_name': column_name\n",
    "    }\n",
    "    df_compare = pd.DataFrame(reference)\n",
    "\n",
    "    for year in [2017, 2018, 2019]:\n",
    "        cols_year = {\n",
    "            'Col_'+str(year): df_shorten(year, \"kharif\")[column_name].unique()\n",
    "        }\n",
    "        df_year = pd.DataFrame(cols_year)\n",
    "        df_compare = df_compare.merge(df_year, left_on=\"Reference\", right_on=\"Col_\"+str(year), suffixes=('', ''))\n",
    "\n",
    "    # Compare values accross tables and show values that are different accross columns\n",
    "    cols_to_check = [\"Reference\", \"Col_2017\", \"Col_2018\", \"Col_2019\"]\n",
    "    df_compare['check'] = df_compare.apply(lambda x: check_cols_similarities(x[cols_to_check], cols_to_check), axis=1)\n",
    "    df_compare[~df_compare['check']]\n",
    "    \n",
    "    return df_compare[~df_compare['check']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "111f52be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Col_name</th>\n",
       "      <th>Col_2017</th>\n",
       "      <th>Col_2018</th>\n",
       "      <th>Col_2019</th>\n",
       "      <th>check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Reference, Col_name, Col_2017, Col_2018, Col_2019, check]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check administrative values for Kharif Season\n",
    "cols_to_check = ['State', 'District', 'Sub-District', 'Block', 'GP']\n",
    "df_result = pd.DataFrame()\n",
    "\n",
    "for col in cols_to_check:\n",
    "    df_result = pd.concat([df_result, compare_table(col, \"kharif\")])\n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f03e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference</th>\n",
       "      <th>Col_name</th>\n",
       "      <th>Col_2017</th>\n",
       "      <th>Col_2018</th>\n",
       "      <th>Col_2019</th>\n",
       "      <th>check</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Reference, Col_name, Col_2017, Col_2018, Col_2019, check]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check administrative values for Rabi Season\n",
    "cols_to_check = ['State', 'District', 'Sub-District', 'Block', 'GP']\n",
    "df_result = pd.DataFrame()\n",
    "\n",
    "for col in cols_to_check:\n",
    "    df_result = pd.concat([df_result, compare_table(col, \"rabi\")])\n",
    "    \n",
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b204ff6",
   "metadata": {},
   "source": [
    "<p>Once it is certain that there are no cases differences and/or typor on the identification columns, then proceed to the next step of creating identifier</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9f62b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique identifier for each data points based on its administrative features, season and crops\n",
    "\n",
    "for season in [\"kharif\", \"rabi\"]:\n",
    "    for year in [2017, 2018, 2019]:\n",
    "        df_shorten(year, season)[\"ID\"] = \"ID\"\n",
    "        for col in cols_ID:\n",
    "            df_shorten(year, season)[\"ID\"] = df_shorten(year, season)[\"ID\"] + \"|\" + df_shorten(year, season)[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e60288d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to update dataset\n",
    "def update_data(old_data, new_data):\n",
    "    if ~np.isnan(new_data):\n",
    "        return new_data\n",
    "    else:\n",
    "        return old_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9fef650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set filenames and format\n",
    "kharif_merged_filename = \"datasets_merged/df_kharif_merged_full.csv\"\n",
    "rabi_merged_filename = \"datasets_merged/df_rabi_merged_full.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ab3283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018 kharif Area Sown (Ha)\n",
      "2018 kharif Sum Insured (Inr)\n",
      "2018 kharif 2016 Yield\n",
      "2018 kharif 2014 Yield\n",
      "2018 kharif SI Per Ha (Inr/Ha)\n",
      "2018 kharif 2013 Yield\n",
      "2018 kharif 2011 Yield\n",
      "2018 kharif 2003 Yield\n",
      "2018 kharif 2004 Yield\n",
      "2018 kharif 2015 Yield\n",
      "2018 kharif 2010 Yield\n",
      "2018 kharif Area Insured (Ha)\n",
      "2018 kharif 2012 Yield\n",
      "2018 kharif 2002 Yield\n",
      "2018 kharif Indemnity Level\n",
      "2018 kharif 2006 Yield\n",
      "2018 kharif 2008 Yield\n",
      "2018 kharif 2007 Yield\n",
      "2018 kharif 2005 Yield\n",
      "2018 kharif 2009 Yield\n",
      "2019 kharif Area Sown (Ha)\n",
      "2019 kharif Sum Insured (Inr)\n",
      "2019 kharif 2016 Yield\n",
      "2019 kharif 2014 Yield\n",
      "2019 kharif SI Per Ha (Inr/Ha)\n",
      "2019 kharif 2013 Yield\n",
      "2019 kharif 2011 Yield\n",
      "2019 kharif 2003 Yield\n",
      "2019 kharif 2004 Yield\n",
      "2019 kharif 2015 Yield\n",
      "2019 kharif 2017 Yield\n",
      "2019 kharif 2010 Yield\n",
      "2019 kharif Area Insured (Ha)\n",
      "2019 kharif 2001 Yield\n",
      "2019 kharif 2012 Yield\n",
      "2019 kharif 2002 Yield\n",
      "2019 kharif Indemnity Level\n",
      "2019 kharif 2006 Yield\n",
      "2019 kharif 2008 Yield\n",
      "2019 kharif 2000 Yield\n",
      "2019 kharif 2007 Yield\n",
      "2019 kharif 2005 Yield\n",
      "2019 kharif 2009 Yield\n",
      "2019 kharif 2018 Yield\n",
      "2018 rabi Area Sown (Ha)\n",
      "2018 rabi Sum Insured (Inr)\n",
      "2018 rabi 2016 Yield\n",
      "2018 rabi 2014 Yield\n",
      "2018 rabi SI Per Ha (Inr/Ha)\n",
      "2018 rabi 2013 Yield\n",
      "2018 rabi 2011 Yield\n",
      "2018 rabi 2003 Yield\n",
      "2018 rabi 2004 Yield\n",
      "2018 rabi 2015 Yield\n",
      "2018 rabi 2010 Yield\n",
      "2018 rabi Area Insured (Ha)\n",
      "2018 rabi 2012 Yield\n",
      "2018 rabi 2002 Yield\n",
      "2018 rabi Indemnity Level\n",
      "2018 rabi 2006 Yield\n",
      "2018 rabi 2008 Yield\n",
      "2018 rabi 2007 Yield\n",
      "2018 rabi 2005 Yield\n",
      "2018 rabi 2009 Yield\n",
      "2019 rabi Area Sown (Ha)\n",
      "2019 rabi Sum Insured (Inr)\n",
      "2019 rabi 2016 Yield\n",
      "2019 rabi 2014 Yield\n",
      "2019 rabi SI Per Ha (Inr/Ha)\n",
      "2019 rabi 2013 Yield\n",
      "2019 rabi 2011 Yield\n",
      "2019 rabi 2003 Yield\n",
      "2019 rabi 2004 Yield\n",
      "2019 rabi 2015 Yield\n",
      "2019 rabi 2010 Yield\n",
      "2019 rabi Area Insured (Ha)\n",
      "2019 rabi 2001 Yield\n",
      "2019 rabi 2012 Yield\n",
      "2019 rabi 2002 Yield\n",
      "2019 rabi Indemnity Level\n",
      "2019 rabi 2006 Yield\n",
      "2019 rabi 2008 Yield\n",
      "2019 rabi 2000 Yield\n",
      "2019 rabi 2007 Yield\n",
      "2019 rabi 2005 Yield\n",
      "2019 rabi 2009 Yield\n"
     ]
    }
   ],
   "source": [
    "# Join older dataset with newer dataset by ID, then update values with the later dataset (if any)\n",
    "# Set older dataset as base\n",
    "for season in [\"kharif\", \"rabi\"]:\n",
    "    df_base = df_shorten(2017, season)\n",
    "    for year in [2018, 2019]:\n",
    "\n",
    "        df_base_cols = df_base.columns.tolist()\n",
    "\n",
    "        # Define newer dataset\n",
    "        df_new = df_shorten(year, season)\n",
    "        df_new_cols = df_new.columns.tolist()\n",
    "\n",
    "        # Create list of columns to update\n",
    "        df_cols = list(set(df_base_cols) & set(df_new_cols))\n",
    "        cols_ID = ['ID', 'State', 'Cluster', 'District', 'Sub-District', 'Block', 'GP', 'Season', 'Crop']\n",
    "\n",
    "        for col in cols_ID:\n",
    "            df_cols.remove(col)\n",
    "\n",
    "        # Merge old and new dataset based on identifier\n",
    "        df_base = pd.merge(left=df_base, right=df_new,\n",
    "                left_on='ID', right_on='ID',\n",
    "                how='outer',\n",
    "                suffixes=(\"\",\"_new\")\n",
    "        )\n",
    "\n",
    "        # Update base dataframe values with newer database values and remove residual columns\n",
    "        for col in df_cols:\n",
    "            print(year, season, col)\n",
    "            df_base[col] = df_base.apply(lambda x: update_data(x[col], x[col+\"_new\"]), axis=1)\n",
    "            df_base = df_base.drop(col+\"_new\", axis = 1)\n",
    "        \n",
    "        df_base = df_base.drop(['State_new', 'Cluster_new', 'District_new', 'Sub-District_new', 'Block_new', 'GP_new', 'Season_new', 'Crop_new'], axis = 1)\n",
    "    \n",
    "    df_base['ID2'] = df_base['ID']\n",
    "    df_base[cols_ID] = df_base['ID'].str.split('|',expand=True)\n",
    "    df_base['ID'] = df_base['ID2']\n",
    "    df_base = df_base.drop(['ID2'], axis = 1)\n",
    "   \n",
    "    # Merge States datasets and export to csv before data exploratory\n",
    "    if season == \"kharif\":\n",
    "        df_base.to_csv(kharif_merged_filename, sep=';')\n",
    "    else:\n",
    "        df_base.to_csv(rabi_merged_filename, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
