{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8crOtmMT6NQh",
    "outputId": "4741346a-ee1a-4d3d-8bdc-003b236533e3"
   },
   "source": [
    "<img src = \"https://escp.eu/sites/default/files/logo/ESCP-logo-white-misalign.svg\" width = 400 style=\"background-color: #240085;\">\n",
    "<h1 align=center><font size = 6>ESCP Business School</font></h1>\n",
    "<h3 align=center><font size = 5>SCOR Datathon</font><br/>\n",
    "<font size = 3>The Data Science Challenge Bridging Indian Agricultureal Protection Gap</font></h3>\n",
    "<h6 align=center>Additional Data - Web Scraping (climate-data.org)</h6>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last Updated: February 15, 2022\\\n",
    "Author: Group 21 - Anniek Brink, Jeanne Dubois, and Resha Dirga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chapter Objectives</h3>\n",
    "\n",
    "<p>As part of the analysis, we will utilise external data, such as: Temprature, Humidity, Sun hours, etc. These information is obtained from websites that available publicly. Thus, this chapter will automate the information obtaining process using web-scraping method.</p>\n",
    "\n",
    "<p><i><u>Note:</u></i> Since every website is uniquely coded, web scraping document will be dedicated to retrieve information from a dedicated website: <a href=\"https://en.climate-data.org/\">en.climate-data.org</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chapter 1: Import modules</h3>\n",
    "<p>This chapter lists all modules that being used on this document. The module import process will be performed on this chapter</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install pandas\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install selenium\n",
    "\n",
    "# Load dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chapter 2: Setups</h3>\n",
    "<p>This chapter prepares the document with key functions, information regarding the driver to use to perfrom web scraping and website links to retrive.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jLyN4EWuLe8Y"
   },
   "outputs": [],
   "source": [
    "# Create function to get response\n",
    "def get_response(url):\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Firefox(options=options, executable_path=geckodriver_path)\n",
    "    driver.get(url)\n",
    "    html = driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup geckodriver_path\n",
    "geckodriver_path = '/Users/admin/Downloads/geckodriver' # This path will need to be modified for each PC with the location of the geckodriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset containing links for webscraping\n",
    "df = pd.read_csv('cities_climate/link_to_extract.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the links from climate-data.org to be retrieved\n",
    "list_link_region = df['Link'].dropna()\n",
    "list_link_region = [k for k in list_link_region if 'climate-data.org' in k]\n",
    "link_to_retrieve = list_link_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.climate-data.org/asia/india/warangal/warangal-968182/',\n",
       " 'https://en.climate-data.org/asia/india/khammam/khammam-4940/']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_to_retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chapter 3: Web scraping</h3>\n",
    "<p>This chapter performs web scraping from the links that has been defined on the Setup chapter.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h0/vgjkl_895611pw1tzh74wq280000gn/T/ipykernel_2772/1182470278.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Firefox(options=options, executable_path=geckodriver_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2\n",
      "Data extraction complete!\n"
     ]
    }
   ],
   "source": [
    "# Extract data\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "list_states = []\n",
    "list_cities = []\n",
    "list_months = []\n",
    "list_avg_temps = []\n",
    "list_min_temps = []\n",
    "list_max_temps = []\n",
    "list_rainfalls = []\n",
    "list_humidities = []\n",
    "list_rainy_days = []\n",
    "list_sun_hours = []\n",
    "list_index = []\n",
    "\n",
    "for link in link_to_retrieve:\n",
    "    print(str(link_to_retrieve.index(link)+1) + '/' + str(len(link_to_retrieve)))\n",
    "    url = link\n",
    "    try:\n",
    "        response = get_response(url)\n",
    "        response_zoom = response.select('table[id=\"weather_table\"]')[0]\n",
    "        response_zoom = response_zoom.find_all('tr')[1:]\n",
    "        cities = url.rsplit('/',2)[1].rsplit('-',1)[0]\n",
    "        cities = [cities] * len(months)\n",
    "        states = url.rsplit('/',3)[1]\n",
    "        states = [states] * len(months)\n",
    "        indexes = [link] * len(months)\n",
    "        \n",
    "        list_states = list_states + states\n",
    "        list_cities = list_cities + cities\n",
    "        list_months = list_months + months\n",
    "        list_index = list_index + indexes\n",
    "    \n",
    "        # Immoweb code            \n",
    "        for tr in response_zoom:           \n",
    "            tds = tr.find_all('td')\n",
    "            for response_text in tds[1:]:\n",
    "                values = response_text.decode_contents()\n",
    "                values = BeautifulSoup(values).select('p')[0].decode_contents()\n",
    "\n",
    "                if tds[0].text == 'Avg. Temperature °C (°F)':\n",
    "                    list_avg_temps.append(values)\n",
    "                elif tds[0].text == 'Min. Temperature °C (°F)':\n",
    "                    list_min_temps.append(values)\n",
    "                elif tds[0].text == 'Max. Temperature °C (°F)':\n",
    "                    list_max_temps.append(values)\n",
    "                elif tds[0].text == 'Precipitation / Rainfall mm (in)':\n",
    "                    list_rainfalls.append(values)\n",
    "                elif tds[0].text == 'Humidity(%)':\n",
    "                    list_humidities.append(values)\n",
    "                elif tds[0].text == 'Rainy days (d)':\n",
    "                    list_rainy_days.append(values)\n",
    "                elif tds[0].text == 'avg. Sun hours (hours)':\n",
    "                    list_sun_hours.append(values)\n",
    "        \n",
    "    except:\n",
    "        print('Cannot extract data')\n",
    "        \n",
    "result = {\n",
    "                'State': list_states,\n",
    "                'City': list_cities,\n",
    "                'Month': list_months,\n",
    "                'Avg. Temperature °C (°F)': list_avg_temps,\n",
    "                'Min. Temperature °C (°F)': list_min_temps,\n",
    "                'Max. Temperature °C (°F)': list_max_temps,\n",
    "                'Precipitation / Rainfall mm (in)': list_rainfalls,\n",
    "                'Humidity(%)': list_humidities,\n",
    "                'Rainy days (d)': list_rainy_days,\n",
    "                'avg. Sun hours (hours)': list_sun_hours,\n",
    "                'index': list_index\n",
    "            }\n",
    "\n",
    "df_result = pd.DataFrame(result)\n",
    "print('Data extraction complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export csv as checkpoint for further preprocessing\n",
    "df_result.to_csv('cities_climate/SCOR_Cities_Climate_CDO_add.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Artemis_Data Mining Tests.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
